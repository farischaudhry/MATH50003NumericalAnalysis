# Floating Point Arithmetic


Arithmetic operations on floating-point numbers are  _exact up to rounding_.
There are three basic rounding strategies: round up/down/nearest.
Mathematically we introduce a function to capture the notion of rounding:

**Definition (rounding)** ${\rm fl}^{\rm up}_{σ,Q,S} : \mathbb R \rightarrow F_{σ,Q,S}$ denotes
the function that rounds a real number up to the nearest floating-point number that is greater or equal.
${\rm fl}^{\rm down}_{σ,Q,S} : \mathbb R \rightarrow F_{σ,Q,S}$ denotes
the function that rounds a real number down to the nearest floating-point number that is greater or equal.
${\rm fl}^{\rm nearest}_{σ,Q,S} : \mathbb R \rightarrow F_{σ,Q,S}$ denotes
the function that rounds a real number to the nearest floating-point number. In case of a tie,
it returns the floating-point number whose least significant bit is equal to zero.
We use the notation ${\rm fl}$ when $σ,Q,S$ and the rounding mode are implied by context,
with ${\rm fl}^{\rm nearest}$ being the default rounding mode.
∎


In IEEE arithmetic, the arithmetic operations `+`, `-`, `*`, `/` are defined by the property
that they are exact up to rounding.  Mathematically we denote these operations as
$⊕, ⊖, ⊗, ⊘ : F ⊗ F → F$ as follows:
$$
\begin{align*}
x ⊕ y &:= {\rm fl}(x+y) \\
x ⊖ y &:= {\rm fl}(x - y) \\
x ⊗ y &:= {\rm fl}(x * y) \\
x ⊘ y &:= {\rm fl}(x / y)
\end{align*}
$$
Note also that  `^` and `sqrt` are similarly exact up to rounding.
Also, note that when we convert a Julia command with constants specified by decimal expansions
we first round the constants to floats, e.g., `1.1 + 0.1` is actually reduced to
$$
{\rm fl}(1.1) ⊕ {\rm fl}(0.1)
$$
This includes the case where the constants are integers (which are normally exactly floats
but may be rounded if extremely large).

**Example (decimal is not exact)** The Julia command `1.1+0.1` gives a different result than `1.2`.
This is because ${\rm fl}(1.1) ≠ 1+1/10$ and ${\rm fl}(0.1) ≠ 1/10$ since their
expansion in _binary_ is not finite, but rather:
$$
\begin{align*}
{\rm fl}(1.1) &= (1.0001100110011001100110011001100110011001100110011010)_2 \\
{\rm fl}(0.1) &= 2^{-4} * (1.1001100110011001100110011001100110011001100110011010)_2 \\
              &= (0.00011001100110011001100110011001100110011001100110011010)_2
\end{align*}
$$
Thus when we add them we get
$$
{\rm fl}(1.1) + {\rm fl}(0.1) = (1.0011001100110011001100110011001100110011001100110011\red{1010})_2
$$
where the red digits indicate those beyond the 52 representable in $F_{54}$. In this case we round up and
get
$$
{\rm fl}(1.1) ⊕ {\rm fl}(0.1) = (1.0011001100110011001100110011001100110011001100110100)_2
$$
On the other hand,
$$
{\rm fl}(1.2) = (1.0011001100110011001100110011001100110011001100110011)_2
$$
which differs by 1 bit.
∎

**WARNING (non-associative)** These operations are not associative! E.g. $(x ⊕ y) ⊕ z$ is not necessarily equal to $x ⊕ (y ⊕ z)$.
Commutativity is preserved, at least.



## Bounding errors in floating point arithmetic


When dealing with normal numbers there are some important constants that we will use
to bound errors.

**Definition (machine epsilon/smallest positive normal number/largest normal number)**
_Machine epsilon_ is denoted
$$
ϵ_{{\rm m},S} := 2^{-S}.
$$
When $S$ is implied by context we use the notation $ϵ_{\rm m}$.
The _smallest positive normal number_ is $q = 1$ and $b_k$ all zero:
$$
\min |F_{σ,Q,S}^{\rm normal}| = 2^{1-σ}
$$
where $|A| := \{|x| : x \in A \}$.
The _largest (positive) normal number_ is
$$
\max F_{σ,Q,S}^{\rm normal} = 2^{2^Q-2-σ} (1.11…)_2 = 2^{2^Q-2-σ} (2-ϵ_{\rm m})
$$
∎



We can bound the error of basic arithmetic operations in terms of machine epsilon, provided
a real number is close to a normal number:

**Definition (normalised range)** The _normalised range_ ${\cal N}_{σ,Q,S} ⊂ ℝ$
is the subset of real numbers that lies
between the smallest and largest normal floating-point number:
$$
{\cal N}_{σ,Q,S} := \{x : \min |F_{σ,Q,S}^{\rm normal}| ≤ |x| ≤ \max F_{σ,Q,S}^{\rm normal} \}
$$
When $σ,Q,S$ are implied by context we use the notation ${\cal N}$.
∎

We can use machine epsilon to determine bounds on rounding:

**Proposition (round bound)**
If $x \in {\cal N}$ then
$$
{\rm fl}^{\rm mode}(x) = x (1 + δ_x^{\rm mode})
$$
where the _relative error_ is
$$
\begin{align*}
|δ_x^{\rm nearest}| &≤ {ϵ_{\rm m} \over 2} \\
|δ_x^{\rm up/down}| &< {ϵ_{\rm m}}.
\end{align*}
$$

**Proof**

We have

∎


This immediately implies relative error bounds on all IEEE arithmetic operations, e.g.,
if $x+y \in {\cal N}$ then
we have
$$
x ⊕ y = (x+y) (1 + δ_1)
$$
where (assuming the default nearest rounding) $|δ_1| ≤ {ϵ_{\rm m} \over 2}.$

## A simplified model of floating point

With a complicated formula it is inelegant to work with normalised ranges. Extending the
bounds to subnormal numbers is tedious, rarely relevant, and beyond the scope of this module. Thus to avoid this
issue we will work with an alternative mathematical model:

**Definition (normal floating point)**
A mathematical model of floating point which the only subnormal number is zero can be defined as:
$$
F_{∞,S} := \{± 2^q × (1.b_1b_2b_3…b_S)_2 :  q ∈ ℤ \} ∪ \{0\}
$$
∎

Note that $F^{\rm normal}_{σ,Q,S} ⊂ F_{∞,S}$ for all $σ,Q ∈ ℕ$.
The definition of rounding ${\rm fl}^{\rm up} : ℝ → F_{∞,S}$ (etc.) naturllay extend to $F_{∞,S}$
and hence we can consider bounds for floating point operations such as $⊕$, $⊖$, etc. And
in this model the round bound is valid for all real numbers (including $x = 0$).






**Example (bounding a simple computation)** We show how to bound the error in computing
$$
(1.1 + 1.2) + 1.3
$$
using extended normal floating-point arithmetic $F_{∞,S}$. First note that `1.1` on a computer is in
fact ${\rm fl}(1.1)$, and we will always assume nearest rounding unless otherwise
stated. Thus this computation becomes
$$
({\rm fl}(1.1) ⊕ {\rm fl}(1.2)) ⊕ {\rm fl}(1.3)
$$
We will show the _absolute error_ is given by
$$
({\rm fl}(1.1) ⊕ {\rm fl}(1.2)) ⊕ {\rm fl}(1.3) = 3.6 + δ
$$
where $|δ| ≤  11 ϵ_{\rm m}.$
First we find
$$
{\rm fl}(1.1) ⊕ {\rm fl}(1.2) = (1.1(1 + δ_1) + 1.2 (1+δ_2))(1 + δ_3)
 = 2.3 + \underbrace{1.1 δ_1 + 1.2 δ_2 + 2.3 δ_3 + 1.1 δ_1 δ_3 + 1.2 δ_2 δ_3}_{δ_4}.
$$
While $δ_1 δ_3$ and $δ_2 δ_3$ are absolutely tiny in practice
we will bound them rather naïvely by $|ϵ_{\rm m}/2|$. Further we round up constants
in the bounds for simplicity. We thus have the bound
$$
|δ_4| ≤ (1+1+2+1+1) ϵ_{\rm m} = 6ϵ_{\rm m}
$$
Thus the computation becomes
$$
((2.3 + δ_4) + 1.3 (1 + δ_5)) (1 + δ_6) = 3.6 + \underbrace{δ_4 + 1.3 δ_5 + 3.6 δ_6 + δ_4 δ_6  + 1.3 δ_5 δ_6}_{δ_7}
$$
where the _absolute error_ is
$$
|δ_7| ≤ (6 + 1 + 2 + 1 + 1) ϵ_{\rm m} = 11 ϵ_{\rm m}
$$
∎


## Divided differences bound

We can use the bound on floating point arithmetic to deduce a bound on divided differences that
captures the phenomena we observed where the error of divided differences became large as $h → 0$.
We assume that the function we are attempting to differentiate is computing using floating point arithmetic
in a way that achieves relative accuracy. 


**Theorem (divided difference error bound)** Let $f$ be twice-differentiable in a neighbourhood of $x$ and assume that 
$$
 f(x) = f^{\rm FP}(x) + δ_x^f
$$
has uniform absolute accuracy in that neighbourhood, that is:
$$
|δ_x^f| ≤ c ϵ_{\rm m}
$$
for a fixed constant $c ≥ 0$. 
Assume we are working in extended normal floating-point arithmetic $F_{∞,S}$
and, for simplicity, $h = 2^{-n}$ where $n ≤ S$ and $|x| ≤ 1$.
The divided difference approximation satisfies
$$
(f^{\rm FP}(x + h) ⊖ f^{\rm FP}(x)) ⊘ h = f'(x) + δ_{x,h}^{\rm FD}
$$
where 
$$
|δ_{x,h}^{\rm FD}| ≤ {|f'(x)| \over 2} ϵ_{\rm m} + M h +  {4c ϵ_{\rm m} \over h}
$$
for $M = \sup_{x ≤ t ≤ x+h} |f''(t)|$.

**Proof**

We have (noting by our assumptions $x ⊕ h = x + h$ and that dividing by $h$ will only change the exponent so
is exact)
$$
\begin{align*}
(f^{\rm FP}(x + h) ⊖ f^{\rm FP}(x)) ⊘ h &= {f(x + h) +  δ^f_{x+h} - f(x) - δ^f_x \over h} (1 + δ_1) \\
&= {f(x+h) - f(x) \over h} (1 + δ_1) + {δ^f_{x+h}- δ^f_x \over h} (1 + δ_1)
\end{align*}
$$
where $|δ_1| ≤ {ϵ_{\rm m} / 2}$. Applying Taylor's theorem we get 
$$
(f^{\rm FP}(x + h) ⊖ f^{\rm FP}(x)) ⊘ h = f'(x) + \underbrace{f'(x) δ_1 + {f''(t) \over 2} h (1 + \delta_1) + {δ^f_{x+h}- δ^f_x \over h} (1 + δ_1)}_{δ_{x,h}^{\rm FD}}
$$
The bound then follows, using the very pessimistic bound $|1 + δ_1| ≤ 2$.

∎

The three-terms of this bound tell us a story: the first term is a fixed (small) error, the second term tends to zero
as $h \rightarrow 0$, while the last term grows like $ϵ_{\rm m}/h$ as $h \rightarrow 0$.  Thus we observe convergence
while the second term dominates, until the last term takes over.
Of course, a bad upper bound is not the same as a proof that something grows, but it is a good indication of 
what happens _in general_ and suffices to motivate the following heuristic to balance the two sources of errors:


**Heuristic (divided difference with floating-point step)** Choose $h$ proportional to $\sqrt{ϵ_{\rm m}}$
in divided differences  so that $M h$ and ${4c \ensuremath{\epsilon}_{\rm m} \over h}$ are roughly the same magnitude.

In the case of double precision $\sqrt{ϵ_{\rm m}} ≈ 1.5\times 10^{-8}$, which is close to when the observed error begins to increase
in the examples we saw before. 



**Remark** While divided differences is of debatable utility for computing derivatives, it is extremely effective
in building methods for solving differential equations, as we shall see later. It is also very useful as a “sanity check"
if one wants something to compare with other numerical methods for differentiation.

