
\section{Rectangular rule}
One possible definition for an integral is the limit of a Riemann sum, for example:
\[
  \ensuremath{\int}_a^b f(x) {\rm d}x = \lim_{n \ensuremath{\rightarrow} \ensuremath{\infty}} h \ensuremath{\sum}_{k=1}^n f(x_k)
\]
where $x_k = a+kh$ are evenly spaced points dividing up the interval $[a,b]$, that is  wit the \{{\textbackslash}it step size\} $h = (b-a)/n$. This suggests an algorithm known as the \emph{(right-sided) rectangular rule} for approximating an integral: choose $n$ large so that
\[
  \ensuremath{\int}_a^n f(x) {\rm d}x \ensuremath{\approx} h \ensuremath{\sum}_{k=1}^n f(x_k).
\]
In the lab we explore practical implementation of this approximation, and observe that the error in approximation is bounded by $C/n$ for some constant $C$. This can be expressed using "Big-O" notation:
\[
\ensuremath{\int}_a^b f(x) {\rm d}x = h \ensuremath{\sum}_{k=1}^n f(x_k) + O(1/n).
\]
In these notes we consider the "Analysis" part of "Numerical Analysis": we want to \emph{prove} the convergence rate of the approximation, including finding an explicit expression for the constant $C$.

To tackle this question we consider the error incurred on a single "rectangle", then sum up the errors on rectangles.

Now for a secret. There are only so many tools available in analysis (especially at this stage of your career), and  one can make a safe bet that the right tool in any analysis proof is either (1) integration-by-parts, (2) geometric series or (3) Taylor series. In this case we use (1):

\begin{lemma}[Rectangular Rule error on one panel] Assuming $f$ is differentiable we have
\[
\ensuremath{\int}_a^b f(x) {\rm d}x = (b-a) f(a) +  \ensuremath{\delta}
\]
where $|\ensuremath{\delta}| \ensuremath{\leq} M (b-a)^2$ for $M = \sup_{a \ensuremath{\leq} x \ensuremath{\leq} b}|f'(x)|$.

\end{lemma}
\textbf{Proof} We write
\meeq{
\ensuremath{\int}_a^b f(x) {\rm d}x = \ensuremath{\int}_a^b (x-a)' f(x)  {\rm d}x = [(x-a) f(x)]_a^b - \ensuremath{\int}_a^b (x-a) f'(x) {\rm d} x \ccr
= (b-a) f(b) + \underbrace{\left(-\ensuremath{\int}_a^b (x-a) f'(x) {\rm d} x \right)}_\ensuremath{\delta}.
}
Recall that we can bound the absolute value of an integral by the sepremum of the integrand times the width of the integration interval:
\[
\abs{\ensuremath{\int}_a^b g(x) {\rm d} x} \ensuremath{\leq} (b-a) \sup_{a \ensuremath{\leq} x \ensuremath{\leq} b}|g(x)|.
\]
The lemma thus follows since
\[
\abs{\ensuremath{\int}_a^b (x-a) f'(x) {\rm d} x} \ensuremath{\leq} (b-a) \sup_{a \ensuremath{\leq} x \ensuremath{\leq} b}|(x-a) f'(x)| \ensuremath{\leq} M (b-a)^2.
\]
\ensuremath{\QED}

Now summing up the errors in each panel gives us the error of using the Rectangular rule:

\begin{theorem}[Rectangular Rule error] Assuming $f$ is differentiable we have
\[
\ensuremath{\int}_a^b f(x) {\rm d}x =  h \ensuremath{\sum}_{k=1}^n f(x_k) +  \ensuremath{\delta}
\]
where $|\ensuremath{\delta}| \ensuremath{\leq} M (b-a) h$ for $M = \sup_{a \ensuremath{\leq} x \ensuremath{\leq} b}|f'(x)|$, $h = (b-a)/n$ and $x_k = a + kh$. 

\end{theorem}
\textbf{Proof} We split the integral into a sum of smaller integrals:
\[
\ensuremath{\int}_a^b f(x) {\rm d}x = \ensuremath{\sum}_{k=1}^n  \ensuremath{\int}_{x_{k-1}}^{x_k} f(x) {\rm d}x =
\ensuremath{\sum}_{k=1}^n  \br[(x_k - x_{k-1}) f(x_k) + \ensuremath{\delta}_k] =  h \ensuremath{\sum}_{k=1}^n f(x_k) +  \underbrace{\ensuremath{\sum}_{k=1}^n \ensuremath{\delta}_k}_\ensuremath{\delta}
\]
where $\ensuremath{\delta}_k$, the error on each panel as in the preceding lemma, satisfies 
\[
|\ensuremath{\delta}_k| \ensuremath{\leq} (x_k-x_{k-1})^2 \sup_{x_{k-1} \ensuremath{\leq} x \ensuremath{\leq} x_k}|f'(x)| \ensuremath{\leq} M h^2.
\]
Thus using the triangular inequality we have 
\[
|\ensuremath{\delta}| = \abs{ \ensuremath{\sum}_{k=1}^n \ensuremath{\delta}_k} \ensuremath{\leq} \ensuremath{\sum}_{k=1}^n |\ensuremath{\delta}_k| \ensuremath{\leq} M n h^2 = M(b-a)h.
\]
\ensuremath{\QED}

Note a consequence of this lemma is that the approximation converges as $n \ensuremath{\rightarrow} 0$. In the labs and problem sheets we will consider the left-sided rule:
\[
\ensuremath{\int}_a^b f(x) {\rm d}x \ensuremath{\approx}  h \ensuremath{\sum}_{k=0}^{n-1} f(x_k).
\]
We also consider the \emph{Trapezium rule}. Here we approximate an integral  by an affine function:
\[
\ensuremath{\int}_a^b f(x) {\rm d} x \ensuremath{\approx} \ensuremath{\int}_a^b {(b-x)f(a) + (x-a)f(b) \over b-a} \dx
= {b-a \over 2} \br[f(a) + f(b)].
\]
Subdividing an interval $a = x_0 < x_1 < \ensuremath{\ldots} < x_n = b$ and applying this approximation separately on each subinterval $[x_{k-1},x_k]$, where $h = (b-a)/n$ and $x_k = a + kh$, leads to the approximation
\[
\ensuremath{\int}_a^b f(x) {\rm d}x \ensuremath{\approx}  {h \over 2} f(a) + h \ensuremath{\sum}_{k=1}^{n-1} f(x_k) + {h \over 2} f(b)
\]
We shall see both experimentally and provably that this approximation converges faster than the rectangular rule.



