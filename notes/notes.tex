\documentclass[12pt,a4paper]{book}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{listings}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.2ex}
\let\QED=\blacksquare
\def\bbD{{\mathbb D}}
\def\bbZ{{\mathbb Z}}
\def\emdash{\hbox{---}}
\def\endash{\hbox{--}}
\def\nsubset{\not\subset}
\def\ldq{``}

\hypersetup
       {   pdfauthor = { {{Sheehan Olver}} },
           pdftitle={ {{MATH50003 Numerical Analysis}} },
           colorlinks=TRUE,
           linkcolor=black,
           citecolor=blue,
           urlcolor=blue
       }

\title{ MATH50003 Numerical Analysis }


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\author{ Sheehan Olver }
\renewcommand{\thechapter}{\Roman{chapter}}

\input{somacros}

\begin{document}

\maketitle

\tableofcontents

\chapter{Calculus on a Computer}

In this first chapter we explore the basics of mathematical computing and numerical analysis.
In particular we investigate the following mathematical problems which can not in general be solved exactly:

\begin{enumerate}
\item Integration. General integrals have no closed form expressions. Can we use a computer to approximate the values of definite integrals?
\item Differentiation. Differentiating a formula as in calculus is usually algorithmic, however, it is often needed to compute derivatives without access to an underlying formula, eg,  a function defined only in code. Can we use a computer to approximate derivatives?  A very important application is in Machine Learning, where there is a need to compute gradients to determine the ``right" weights in a neural network. 
\item Root finding. There is no general formula for finding roots (zeros) of arbitrary functions, or even polynomials that are of degree 5 (quintics) or higher. Can we compute roots of general functions using a computer?
\end{enumerate}

In this chapter we discuss:

\begin{enumerate}
\item I.1 Rectangular rule: we review the rectangular rule for integration and deduce the {\it converge rate} of the approximation. In the lab/problem sheet  we investigate its implementation as well as extensions to the Trapezium rule. 
\item I.2 Divided differences: we investigate approximating derivatives by a divided difference and again deduce the convergence rates. In the lab/problem sheet we extend the approach to the central differences formula and computing second derivatives. We also observe a mystery: the approximations may have significant errors in practice, and there is a limit to the accuracy.
\item I.3 Dual numbers: we introduce the algebraic notion of a {\it dual number} which allows the implemention of {\it forward-mode automatic differentiation}, a high accuracy alternative to divided differences for computing derivatives.
\item I.4 Newton's method: Newton's method is a basic approach for computing roots/zeros of a function. We use dual numbers to implement this algorithm.
\end{enumerate}



\input{I.1.RectangularRule.tex}
\input{I.2.DividedDifferences.tex}
\input{I.3.DualNumbers.tex}
\input{I.4.NewtonMethod.tex}


\chapter{Representing Numbers}

In this chapter we aim to answer the question: when can we rely on computations done on a computer?  Why are some computations (differentiation via divided differences), extremely inaccurate whilst others (integration via rectangular rule) accurate up to about 16 digits?  In order to address these questions we need to dig deeper and understand at a basic level what a computer is actually doing when manipulating numbers. 

Before we begin it is important to have a basic model of how a computer works. Our simplified model of a computer will consist of a \href{https://en.wikipedia.org/wiki/Central_processing_unit}{Central Processing Unit (CPU)}\ensuremath{\emdash}the  brains of the computer\ensuremath{\emdash}and \href{https://en.wikipedia.org/wiki/Computer_data_storage#Primary_storage}{Memory}\ensuremath{\emdash}where  data is stored. Inside the CPU there are \href{https://en.wikipedia.org/wiki/Processor_register}{registers}, where data is temporarily stored after being loaded from memory, manipulated by the CPU, then stored back to memory. 

Memory is a sequence of bits: \texttt{1}s and \texttt{0}s, essentially ``on/off" switches. These are grouped into bytes, which consist of 8 bits. Each byte has a memory address: a unique number specifying its location in memory. The number of possible addresses is limited by the processor: if a computer has a a $p$-bit CPU then each address is represented by $p$ bits, for a total of $2^p$ addresses (on a modern 64-bit CPU this is $2^{64} \ensuremath{\approx} 1.8 \times 10^{19}$ bytes). Further, each register consists of exactly $p$-bits.


Thus representing numbers on a computer must overcome three fundamental limitations:
\begin{enumerate}
\item CPUs can only manipulate data $p$-bits at a time.
\item Memory is finite, in particular at most $2^p$ bytes.
\item There is no such thing as an ``error'': if anything goes wrong in the computation we must use some of the $p$-bits to indicate this.
\end{enumerate}

This is clearly problematic: there are an infinite number of integers and an uncountable number of reals! Each of which we need to store in precisely $p$-bits. Moreover, some operations are simply undefined, like division by 0.  This chapter discusses the solution to this problem, alongside the mathematical analysis that is needed to understand the implications, in particular, that computations have {\it error}.

In particular we discuss:

\begin{enumerate}
\item II.1 Integers: unsigned (non-negative) and signed integers are representable using exactly $p$-bits by using modular arithmetic in all operations.
\item II.2 Reals:  real numbers are approximated by floating point numbers, which are the computers version of scientific notation.
\item II.3 Floating Point Arithmetic:  arithmetic with floating point numbers is exact up-to-rounding, which introduces small-but-understandable errors in the computations. We explain how these errors can be analysed mathematically to get rigorous bounds. 
\item II.4 Interval Arithmetic: rounding can be controlled in order to implement {\it interval arithmetic}, a way to compute rigorous bounds for computations. In the lab, we use this to compute up to 15 digits of ${\rm e} \equiv \exp 1$ rigorously with precise bounds on the error.
\end{enumerate}


\input{II.1.Integers.tex}
\input{II.2.Reals.tex}

\end{document}