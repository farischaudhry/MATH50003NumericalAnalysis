
\section{Polynomial Interpolation and Regression}
In this section we switch tracks and begin to consider numerical linear algebra related to rectangular matrices and least squares systems, which we motivate with an application to polynomial regression. \emph{Polynomial interpolation} is the process of finding a polynomial that equals data at a precise set of points. A more robust scheme is \emph{polynomial regression} where we use more data than the degrees of freedom in the polynomial. We therefore determine the polynomial using \emph{least squares}: find the polynomial whose samples at the points are as close as possible to the data, as measured in the $2$-norm. This least squares problem is done numerically which will be discussed in the next few sections.

\subsection{Polynomial interpolation}
Our prelimary goal is given a set of points $x_j$ and data $f_j$, usually samples of a function $f_j = f(x_j)$, find a polynomial that interpolates the data at the points:

\begin{definition}[interpolatory polynomial] Given \emph{distinct} points $\ensuremath{\bm{\x}} = \vectt[x_1,\ensuremath{\ldots},x_n] \ensuremath{\in} \ensuremath{\bbF}^n$ and \emph{data} $\ensuremath{\bm{\f}} = \vectt[f_1,\ensuremath{\ldots},f_n] \ensuremath{\in} \ensuremath{\bbF}^n$, a degree $n-1$ \emph{interpolatory polynomial} $p(x)$ satisfies
\[
p(x_j) = f_j
\]
\end{definition}

The easiest way to solve this problem is to invert the Vandermonde system:

\begin{definition}[Vandermonde] The \emph{Vandermonde matrix} associated with $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbF}^m$ is the matrix
\[
V_{\ensuremath{\bm{\x}},n} := \begin{bmatrix} 1 & x_1 & \ensuremath{\cdots} & x_1^{n-1} \\
                    \ensuremath{\vdots} & \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                    1 & x_m & \ensuremath{\cdots} & x_m^{n-1}
                    \end{bmatrix} \ensuremath{\in} \ensuremath{\bbF}^{m \ensuremath{\times} n}.
\]
When it is clear from context we omit the subscripts $\ensuremath{\bm{\x}},n$. \end{definition}

Writing the coefficients of a polynomial
\[
p(x) = \ensuremath{\sum}_{k=0}^{n-1} c_k x^k
\]
as a vector  $\ensuremath{\bm{\c}} = \vectt[c_0,\ensuremath{\ldots},c_{n-1}] \ensuremath{\in} \ensuremath{\bbF}^n$, we note that $V$ encodes the linear map from coefficients to values at a grid, that is,
\[
V\ensuremath{\bm{\c}} = \Vectt[c_0 + c_1 x_1 + \ensuremath{\cdots} + c_{n-1} x_1^{n-1}, \ensuremath{\vdots}, c_0 + c_1 x_m + \ensuremath{\cdots} + c_{n-1} x_m^{n-1}] = \Vectt[p(x_1),\ensuremath{\vdots},p(x_m)].
\]
In the square case (where $m=n$), the coefficients of an interpolatory polynomial are given by $\ensuremath{\bm{\c}} = V^{-1} \ensuremath{\bm{\f}}$, so that
\[
\Vectt[p(x_1),\ensuremath{\vdots},p(x_n)] = V \ensuremath{\bm{\c}} = V V^{-1} \ensuremath{\bm{\f}} = \Vectt[f_1,\ensuremath{\vdots},f_n].
\]
This inversion is justified by the following:

\begin{proposition}[interpolatory polynomial uniqueness] Interpolatory polynomials are unique and therefore square Vandermonde matrices are invertible.

\end{proposition}
\textbf{Proof} Suppose $p$ and $\pt$ are both interpolatory polynomials of the same function. Then $p(x) - \pt(x)$ vanishes at $n$ distinct points $x_j$. By the fundamental theorem of algebra it must be zero, i.e., $p = \pt$.

For the second part, if $V \ensuremath{\bm{\c}} = 0$ for $\ensuremath{\bm{\c}} = \vectt[c_0,\ensuremath{\ldots},c_{n-1}] \ensuremath{\in} \ensuremath{\bbF}^n$ then for $q(x) = c_0 + \ensuremath{\cdots} + c_{n-1} x^{n-1}$ we have
\[
q(x_j) = \ensuremath{\bm{\e}}_j^\ensuremath{\top} V \ensuremath{\bm{\c}} = 0
\]
hence $q$ vanishes at $n$ distinct points and is therefore 0, i.e., $\ensuremath{\bm{\c}} = 0$.

\ensuremath{\QED}

We can invert square Vandermonde matrix numerically in $O(n^3)$ operations using the PLU factorisation. But it turns out we can also construct the interpolatory polynomial directly, and evaluate the polynomial in only $O(n^2)$ operations. We will use the following polynomials which equal $1$ at one grid point and zero at the others:

\begin{definition}[Lagrange basis polynomial] The \emph{Lagrange basis polynomial} is defined as
\[
\ensuremath{\ell}_k(x) := \ensuremath{\prod}_{j \ensuremath{\neq} k} {x-x_j \over x_k - x_j} =  {(x-x_1) \ensuremath{\cdots}(x-x_{k-1})(x-x_{k+1}) \ensuremath{\cdots} (x-x_n) \over (x_k - x_1) \ensuremath{\cdots} (x_k - x_{k-1}) (x_k - x_{k+1}) \ensuremath{\cdots} (x_k - x_n)}
\]
\end{definition}

Plugging in the grid points verifies that: $\ensuremath{\ell}_k(x_j) = \ensuremath{\delta}_{kj}$.

We can use these to construct the interpolatory polynomial:

\begin{theorem}[Lagrange interpolation] The unique interpolation polynomial is:
\[
p(x) = f_1 \ensuremath{\ell}_1(x) + \ensuremath{\cdots} + f_n \ensuremath{\ell}_n(x)
\]
\end{theorem}
\textbf{Proof} Note that
\[
p(x_j) = \ensuremath{\sum}_{j=1}^n f_j \ensuremath{\ell}_k(x_j) = f_j.
\]
\ensuremath{\QED}

\begin{example}[interpolating an exponential] We can interpolate $\exp(x)$ at the points $0,1,2$. That is, our data is $\ensuremath{\bm{\f}} = \vectt[{\rm e}, {\rm e},{\rm e}^2]$ and the interpolatory polynomial is
\begin{align*}
p(x) &= \ensuremath{\ell}_1(x) + {\rm e} \ensuremath{\ell}_2(x) + {\rm e}^2 \ensuremath{\ell}_3(x) =
{(x - 1) (x-2) \over (-1)(-2)} + {\rm e} {x (x-2) \over (-1)} +
{\rm e}^2 {x (x-1) \over 2} \\
&= (1/2 - {\rm e} +{\rm e}^2/2)x^2 + (-3/2 + 2 {\rm e}  - {\rm e}^2 /2) x + 1
\end{align*}
\end{example}

\textbf{Remark} Interpolating at evenly spaced points is a really \emph{bad} idea: interpolation is inheritely ill-conditioned. The labs will explore this issue experimentally. Another serious issue is that monomials are a horrible basis for interpolation. This is intuitive: when $n$ is large $x^n$ is basically zero near the origin and hence $x_j^n$ numerically lose linear independence, that is, on a computer they appear to be linearly dependent (up to rounding errors). We will discuss alternative bases in Part IV.

\subsection{Polynomial regression}
In many settings interpolation is not an accurate or appropriate tool. Data is often on an evenly spaced grid in which case (as seen in the labs) interpolation breaks down catastrophically. Or the data is noisy and one ends up over resolving: approximating the noise rather than the signal. A simple solution is \emph{polynomial regression} use more sample points than than the degrees of freedom in the polynomial. The special case of an affine polynomial is called \emph{linear regression}.

More precisely, for $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbF}^m$ and for $n < m$ we want to find a degree $n-1$ polynomial
\[
p(x) = \ensuremath{\sum}_{k=0}^{n-1} c_k x^k
\]
such that
\[
\Vectt[p(x_1), \ensuremath{\vdots}, p(x_m)] \ensuremath{\approx} \underbrace{\Vectt[f_1,\ensuremath{\vdots},f_m]}_{\ensuremath{\bm{\f}}}.
\]
Mapping between coefficients $\ensuremath{\bm{\c}} \ensuremath{\in} \ensuremath{\bbF}^n$ to polynomial values on a grid can be accomplished  via rectangular Vandermonde matrices. In particular, our goal is to choose $\ensuremath{\bm{\c}} \ensuremath{\in} \ensuremath{\bbF}^n$ so that
\[
V \ensuremath{\bm{\c}}  = \Vectt[p(x_1), \ensuremath{\vdots}, p(x_m)] \ensuremath{\approx} \ensuremath{\bm{\f}}.
\]
We do so by solving the \emph{least squares} system: given $V \ensuremath{\in} \ensuremath{\bbF}^{m \ensuremath{\times} n}$ and $\ensuremath{\bm{\f}} \ensuremath{\in} \ensuremath{\bbF}^m$ we want to find $\ensuremath{\bm{\c}} \ensuremath{\in} \ensuremath{\bbF}^n$ such that
\[
\| V \ensuremath{\bm{\c}} - \ensuremath{\bm{\f}} \|
\]
is minimal. Note interpolation is a special case where this norm is precisely zero (which is indeed minimal), but in general this norm may be rather large.   We will discuss the numerical solution of least squares problems in the next few sections.

\textbf{Remark} Using regression instead of interpolation can overcome the issues with evenly spaced grids. However, the monomial basis is still very problematic.



